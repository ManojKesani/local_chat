**Sentence Embeddings and Their Role in NLP Quiz**
=====================================================

### Quiz 1: What are Sentence Embeddings?

* What are sentence embeddings?
a) Numerical representations of words
b) Numerical representations of sentences
c) Numerical representations of documents
d) Numerical representations of paragraphs

Answer: b) Numerical representations of sentences

### Quiz 2: Role of Sentence Embeddings in NLP

* What is the role of sentence embeddings in NLP?
a) To understand the meaning of individual words
b) To understand the meaning and context of text
c) To analyze the syntax of sentences
d) To generate new sentences

Answer: b) To understand the meaning and context of text

### Quiz 3: From W2VEC to Sentence Embeddings

* What is an example of sentence embeddings using W2VEC?
a) No: [1,0,0,0]
b) I: [0,2,0,0]
c) Am: [-1,0,1,0]
d) All of the above

Answer: d) All of the above

### Quiz 4: Cross-Encoder (BERT) vs. Bi-Encoder (SBERT)

* What is the main difference between Cross-Encoder (BERT) and Bi-Encoder (SBERT)?
a) BERT is suitable for pair regression tasks, while SBERT is more efficient for computing sentence embeddings
b) SBERT is suitable for pair regression tasks, while BERT is more efficient for computing sentence embeddings
c) BERT is more efficient for computing sentence embeddings, while SBERT is suitable for pair regression tasks
d) SBERT is more efficient for computing sentence embeddings, while BERT is suitable for pair regression tasks

Answer: a) BERT is suitable for pair regression tasks, while SBERT is more efficient for computing sentence embeddings

### Quiz 5: Siamese Network Architectures

* What is the main difference between Cross-Encoder Architecture and Bi-Encoder Architecture?
a) Cross-Encoder uses two separate encoders, while Bi-Encoder uses a single encoder
b) Bi-Encoder uses two separate encoders, while Cross-Encoder uses a single encoder
c) Cross-Encoder uses the CLS token as sentence representation, while Bi-Encoder averages the BERT output layer
d) Bi-Encoder uses the CLS token as sentence representation, while Cross-Encoder averages the BERT output layer

Answer: c) Cross-Encoder uses the CLS token as sentence representation, while Bi-Encoder averages the BERT output layer

### Quiz 6: Loss Functions for Siamese Network

* What is the loss function used for training Siamese networks for NLI dataset?
a) Cross-Entropy Loss Function
b) Regression Loss Function (MSE)
c) Triplet Loss Function
d) All of the above

Answer: a) Cross-Entropy Loss Function

### Quiz 7: Applications of BERT and SBERT

* What is an application of BERT (Cross-Encoder)?
a) Semantic Search Engines
b) Customer Support Automation
c) Data Deduplication
d) Real-time Social Media Monitoring

Answer: b) Customer Support Automation

### Quiz 8: Key Points to Understand

* What is a key point to understand about sentence embeddings?
a) They are crucial for understanding the meaning and context of text in NLP
b) They are only used for pair regression tasks
c) They are only used for computing sentence embeddings
d) They are not used in NLP

Answer: a) They are crucial for understanding the meaning and context of text in NLP