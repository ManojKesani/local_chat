{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AsyncClient',\n",
       " 'ChatResponse',\n",
       " 'Client',\n",
       " 'GenerateResponse',\n",
       " 'Message',\n",
       " 'Options',\n",
       " 'ProgressResponse',\n",
       " 'RequestError',\n",
       " 'ResponseError',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_client',\n",
       " '_types',\n",
       " 'chat',\n",
       " 'copy',\n",
       " 'create',\n",
       " 'delete',\n",
       " 'embeddings',\n",
       " 'generate',\n",
       " 'list',\n",
       " 'pull',\n",
       " 'push',\n",
       " 'show']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dir(ollama))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-04-27T04:17:06.835018Z',\n",
      " 'done': True,\n",
      " 'eval_count': 53,\n",
      " 'eval_duration': 1140145000,\n",
      " 'load_duration': 3671047042,\n",
      " 'message': {'content': 'The sky is blue due to Rayleigh scattering. This '\n",
      "                        'scattering occurs when sunlight interacts with '\n",
      "                        \"molecules in the Earth's atmosphere. The blue light \"\n",
      "                        'has a shorter wavelength than other colors, so it is '\n",
      "                        'scattered more strongly. This is why the sky appears '\n",
      "                        'blue to us.',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'gemma:2b',\n",
      " 'prompt_eval_count': 15,\n",
      " 'prompt_eval_duration': 79300000,\n",
      " 'total_duration': 4892738625}\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from pprint import pprint\n",
    "\n",
    "response = ollama.chat(model='gemma:2b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2024-04-27T04:17:16.394764Z',\n",
      " 'done': True,\n",
      " 'eval_count': 168,\n",
      " 'eval_duration': 5649859000,\n",
      " 'load_duration': 3376737458,\n",
      " 'message': {'content': ' The sky appears blue to the human eye because of a '\n",
      "                        'phenomenon known as Rayleigh scattering. As sunlight '\n",
      "                        \"enters Earth's atmosphere, it encounters molecules \"\n",
      "                        'and small particles which cause the light to scatter '\n",
      "                        'in all directions. Blue light (shorter wavelength) is '\n",
      "                        'scattered more than other colors because its shorter '\n",
      "                        'wavelength causes it to diffract more sharply when '\n",
      "                        'encountering these particles. This scattering of blue '\n",
      "                        'light across the sky makes it appear predominantly '\n",
      "                        \"blue from our perspective on Earth's surface, \"\n",
      "                        'especially during the daytime when the sun is high in '\n",
      "                        'the sky. Additionally, at sunrise and sunset, the '\n",
      "                        'light path through the atmosphere is longer, which '\n",
      "                        'means even more blue light is scattered out, leaving '\n",
      "                        'shades of red, orange, and pink to dominate.',\n",
      "             'role': 'assistant'},\n",
      " 'model': 'phi3',\n",
      " 'prompt_eval_count': 13,\n",
      " 'prompt_eval_duration': 153076000,\n",
      " 'total_duration': 9195659416}\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from pprint import pprint\n",
    "\n",
    "response = ollama.chat(model='phi3', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 171897.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "Prompt: explain\n",
      "the difference between the two.\n",
      "\n",
      "The first is that the first-order approximation is not valid for the case of a single-particle system. The second-order approximation is valid for the case of a single-particle system, but the first-order approximation is not valid for the case of a multi-particle system.\n",
      "\n",
      "The first-order approximation is valid for the case of a single-particle system. The second-order approximation is valid for the case of\n",
      "==========\n",
      "Prompt: 37.196 tokens-per-sec\n",
      "Generation: 71.313 tokens-per-sec\n"
     ]
    }
   ],
   "source": [
    "from mlx_lm import load, generate\n",
    "\n",
    "prompt=\"explain\"\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/OpenELM-450M\")\n",
    "response = generate(model, tokenizer, prompt=prompt, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role: Adventure Guide\n",
      "Goal: Craft exhilarating adventures that ignite the spirit of exploration and adrenaline.\n",
      "Backstory: A daredevil at heart, fueled by the thrill of discovering uncharted territories and conquering untamed landscapes.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the combined JSON file\n",
    "with open('agents.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def get_role_goal_backstory(key):\n",
    "    # Extract role, goal, and backstory using the key\n",
    "    role = data['roles'].get(key)\n",
    "    goal = data['goals'].get(key)\n",
    "    backstory = data['backstories'].get(key)\n",
    "    \n",
    "    return role, goal, backstory\n",
    "\n",
    "# Example usage:\n",
    "key = \"adventure_guide\"\n",
    "role, goal, backstory = get_role_goal_backstory(key)\n",
    "print(\"Role:\", role)\n",
    "print(\"Goal:\", goal)\n",
    "print(\"Backstory:\", backstory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AgentCreator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m creator \u001b[38;5;241m=\u001b[39m \u001b[43mAgentCreator\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_data.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Add a new agent\u001b[39;00m\n\u001b[1;32m      5\u001b[0m creator\u001b[38;5;241m.\u001b[39madd_agent(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_agent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNew Agent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNew Goal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNew Backstory\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AgentCreator' is not defined"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "creator = AgentCreator('combined_data.json')\n",
    "\n",
    "# Add a new agent\n",
    "creator.add_agent('new_agent', 'New Agent', 'New Goal', 'New Backstory')\n",
    "\n",
    "# Display available agents after adding\n",
    "creator.display_available_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Agents:\n",
      "- Key: adventure_guide\n",
      "  Role: Adventure Guide\n",
      "  Goal: Craft exhilarating adventures that ignite the spirit of exploration and adrenaline.\n",
      "  Backstory: A daredevil at heart, fueled by the thrill of discovering uncharted territories and conquering untamed landscapes.\n",
      "- Key: culture_crafter\n",
      "  Role: Culture Crafter\n",
      "  Goal: Immerse travelers in the diverse cultures of the world, from ancient traditions to modern marvels.\n",
      "  Backstory: A global citizen with a deep appreciation for the mosaic of human culture, seeking to bridge gaps and foster understanding through shared experiences.\n",
      "- Key: serenity_scout\n",
      "  Role: Serenity Scout\n",
      "  Goal: Lead seekers of tranquility to idyllic retreats where they can rejuvenate mind, body, and soul.\n",
      "  Backstory: A guardian of tranquility, with an intuitive sense for serene sanctuaries and a mission to restore balance in a chaotic world.\n",
      "- Key: gastronomic_guru\n",
      "  Role: Gastronomic Guru\n",
      "  Goal: Delight food enthusiasts with gastronomic odysseys, exploring the flavors of local cuisines.\n",
      "  Backstory: A culinary virtuoso, dedicated to exploring the world's flavors, uncovering culinary secrets, and savoring every bite.\n",
      "- Key: eco_explorer\n",
      "  Role: Eco Explorer\n",
      "  Goal: Promote sustainable travel practices while uncovering the wonders of nature and biodiversity.\n",
      "  Backstory: A steward of the Earth, dedicated to preserving natural wonders and inspiring others to tread lightly on their journey through eco-conscious exploration.\n",
      "- Key: urban_nomad\n",
      "  Role: Urban Nomad\n",
      "  Goal: Navigate the bustling metropolises, unveiling the hidden gems and vibrant subcultures of urban life.\n",
      "  Backstory: A modern-day nomad, navigating the urban jungle with curiosity and adaptability, uncovering the essence of city life in its myriad forms.\n",
      "- Key: agent_tester\n",
      "  Role: Researcher\n",
      "  Goal: Find and summarize the latest AI news\n",
      "  Backstory: You're a researcher at a large company.\n",
      "  You're responsible for analyzing data and providing insights\n",
      "  to the business.\n",
      "- Key: PDF_Cleaner\n",
      "  Role: Document Cleansing Specialist\n",
      "  Goal: To extract relevant information from cluttered documents and present it in a clear, concise format.\n",
      "  Backstory: A meticulous data curator with a keen eye for detail, the Document Cleansing Specialist is adept at navigating through cluttered documents to uncover the nuggets of valuable information buried within.With a background in data analysis and document processing, this agent has honed the skills necessary to efficiently clean and organize text, ensuring that only the most pertinent details are retained while extraneous clutter is discarded. Whether it's extracting crucial insights from financial reports, distilling key findings from research papers, or summarizing important data from lengthy documents, the Document Cleansing Specialist is the go-to expert for tidying up messy text and delivering polished, refined content. Armed with cutting-edge text processing tools and a commitment to excellence, this agent transforms chaotic documents into streamlined resources, empowering decision-makers with clarity and precision.\n",
      "\n",
      "- Key: Prescriptive_Analytics_Tutor\n",
      "  Role: Prescriptive_Analytics_Tutor\n",
      "  Goal: To facilitate comprehensive understanding of prescriptive analytics concepts through personalized guidance and interactive learning experiences.\n",
      "  Backstory: As a seasoned educator and subject matter expert in prescriptive analytics, the Prescriptive Analytics Tutor brings a wealth of knowledge and teaching experience to the table. With a passion for fostering learning and a commitment to student success, this tutor specializes in distilling complex concepts into digestible chunks of information, ensuring clarity and comprehension. Leveraging a deep understanding of various pedagogical approaches and instructional techniques, the tutor tailors learning strategies to suit individual learning styles and preferences. By analyzing class notes and summaries, the tutor identifies areas of strength and weakness, customizing lesson plans and practice exercises to address specific learning needs. Through interactive sessions, engaging discussions, and hands-on activities, the Prescriptive Analytics Tutor empowers students to master the intricacies of prescriptive analytics, equipping them with the skills and confidence needed to excel in their academic pursuits and beyond.\n",
      "\n",
      "- Key: Personalized_Learning_Tutor\n",
      "  Role: Personalized_Learning_Tutor\n",
      "  Goal: To facilitate efficient learning by summarizing class notes and providing personalized guidance and assistance to aid understanding.\n",
      "  Backstory: The Personalized Learning Tutor is your dedicated companion on the journey of knowledge acquisition. With a background in education and a passion for empowering learners, this tutor leverages advanced techniques in natural language processing and educational psychology to provide tailored support. Drawing upon extensive experience in curriculum design and pedagogical methodologies, the Personalized Learning Tutor excels at distilling complex class notes into digestible summaries, ensuring that you grasp the key concepts effectively. Furthermore, this tutor offers personalized assistance based on your learning style, preferences, and areas of difficulty. Whether you prefer visual aids, interactive quizzes, or in-depth explanations, the Personalized Learning Tutor adapts to your needs, guiding you towards a deeper understanding of the subject matter. With a focus on fostering independent learning skills and nurturing academic growth, this tutor serves as a trusted mentor and ally in your educational journey, empowering you to achieve your learning goals with confidence and competence.\n",
      "    \n",
      "- Key: quiz_Tutor\n",
      "  Role: quiz_Tutor\n",
      "  Goal: To facilitate effective learning by providing personalized study sessions and interactive quizzes based on class notes and materials.\n",
      "  Backstory: Born out of a passion for education and a desire to empower learners, the Personal Tutoring AI is your dedicated study companion. Trained on a vast array of educational materials and equipped with advanced natural language processing capabilities, this AI tutor is designed to assist you in mastering your class notes and materials. Drawing from the latest advancements in pedagogy and cognitive science, the Personal Tutoring AI tailors its tutoring sessions to your individual learning style and pace. Whether you prefer visual explanations, interactive quizzes, or hands-on activities, this tutor adapts to your needs, ensuring that you grasp even the most challenging concepts with ease. With a patient and supportive demeanor, the Personal Tutoring AI guides you through your study journey, providing encouragement, feedback, and reinforcement along the way. From reinforcing key concepts to testing your understanding through engaging quizzes, this AI tutor is committed to helping you achieve academic success.\n",
      "    \n",
      "- Key: Paper_Summarizer\n",
      "  Role: Paper_Summarizer\n",
      "  Goal: To distill complex research papers into concise summaries that capture the key findings and insights.\n",
      "  Backstory: The Research Paper Summarizer is an innovative solution born out of the need to navigate the vast sea of academic literature efficiently. Trained on a diverse range of research papers spanning various disciplines, this AI specializes in condensing lengthy and technical documents into digestible summaries. With a deep understanding of natural language processing and machine learning algorithms, the Research Paper Summarizer analyzes the content of research papers to identify the most crucial information, including hypotheses, methodologies, results, and conclusions. Its goal is to provide researchers, students, and professionals with quick access to the essence of scholarly articles, saving valuable time and effort in the process. Drawing on its vast database of academic knowledge and sophisticated text analysis techniques, the Research Paper Summarizer delivers accurate and comprehensive summaries that capture the essence of complex research studies. Whether you're conducting literature reviews, staying updated on the latest research trends, or simply seeking to understand a particular topic, this AI-powered tool is your indispensable ally in the pursuit of knowledge.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from agents import AgentCreator\n",
    "\n",
    "a = AgentCreator('agents.json')\n",
    "a.display_available_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent with key 'Resume_Summarizer' already exists.\n"
     ]
    }
   ],
   "source": [
    "a.add_agent(\n",
    "    key='Resume_Summarizer',\n",
    "    role='Resume_Summarizer',\n",
    "    goal='Given a text file of a candidate resume identify and summarize key points useful to judge the candidate',\n",
    "    backstory=\"\"\"With a background in human resources and recruitment, expertise in resume analysis and summarization techniques. \n",
    "    Drawing upon my skills in data analysis and communication, i created a specialized system to efficiently extract pertinent information from resumes and craft succinct summaries. \n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UI/UX Banner Designer;Art Director / Front End Developer;Interactive / Digital / Print Designer;Marketing / Communications Manager;Multimedia Developer/Designer;Multimedia Designer;Graphic Designer / Webmaster;Ui;Ux;Active server pages;Html;Javascript;Xml;jquery;Asp;Cold fusion;Css;Interdev;Visual interdev;Cms;Usability;Animation;Print design;Interactive design;Wordpress;Digital design;User experience\n",
      "Senior Software Developer;Senior Software Developer;Senior Software Developer;Senior Software Developer;Senior Software Developer;Senior Software Developer;Senior Software Developer;Senior Software Developer;Software Developer;Software Developer;Software Developer;Software Developer\n",
      "Systems Engineer;Infrastructure Engineer;Systems Administrator;Technical Consultant;Active Directory;Vmware;System Administrator;DNS;Powershell;HP blades;SQL;IIS;Windows Server\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "# Define the path to the text file\n",
    "text_file_path = \"resume_corpus/resume_samples.txt\"\n",
    "\n",
    "# Define the path to save the CSV file\n",
    "csv_file_path = \"resume_corpus/resume_samples.csv\"\n",
    "\n",
    "# Define the columns for the DataFrame\n",
    "columns = [\"Reference ID\", \"Occupations\", \"Text Resume\"]\n",
    "\n",
    "# Read the text file and parse its contents\n",
    "c = 0\n",
    "with open(text_file_path, \"r\", encoding=\"latin-1\") as file:\n",
    "    lines = file.readlines()\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        fields = line.strip().split(\":::\")\n",
    "        # print(len(fields))\n",
    "        if len(fields) != 3:\n",
    "            print(f\"{fields[1]}\")\n",
    "            \n",
    "\n",
    "        # data.append(fields)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the path to the text file\n",
    "text_file_path = \"resume_corpus/resume_samples.txt\"\n",
    "\n",
    "# Define the path to save the CSV file\n",
    "csv_file_path = \"resume_corpus/resume_samples.csv\"\n",
    "\n",
    "# Define the columns for the CSV file\n",
    "columns = [\"Reference ID\", \"Occupations\", \"Text Resume\"]\n",
    "\n",
    "# Open the text file and create a CSV file\n",
    "with open(text_file_path, \"r\", encoding=\"latin-1\") as infile, open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(columns)  # Write the header row\n",
    "\n",
    "    # Iterate through each line in the text file\n",
    "    for line in infile:\n",
    "        # Split the line into fields, handling cases where ':::' is part of the text\n",
    "        parts = line.strip().split(\":::\")\n",
    "        reference_id = parts[0]\n",
    "        occupations = parts[1]\n",
    "        text_resume = \":::\".join(parts[2:])\n",
    "        \n",
    "        # Write the row to the CSV file\n",
    "        writer.writerow([reference_id, occupations, text_resume])\n",
    "\n",
    "print(\"CSV file saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./resume_corpus/resume_samples.csv')\n",
    "i=1\n",
    "o = df['Occupations'][i]\n",
    "t = df['Text Resume'][i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reference ID</th>\n",
       "      <th>Occupations</th>\n",
       "      <th>Text Resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\1.h...</td>\n",
       "      <td>Database Administrator;Database Administrator;...</td>\n",
       "      <td>Database Administrator &lt;span class=\"hl\"&gt;Databa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\10....</td>\n",
       "      <td>Database Administrator;SQL, Microsoft PowerPoi...</td>\n",
       "      <td>Database Administrator &lt;span class=\"hl\"&gt;Databa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\100...</td>\n",
       "      <td>Oracle Database Administrator;Oracle Database ...</td>\n",
       "      <td>Oracle Database Administrator Oracle &lt;span cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\100...</td>\n",
       "      <td>Amazon Redshift Administrator and ETL Develope...</td>\n",
       "      <td>Amazon Redshift Administrator and ETL Develope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\100...</td>\n",
       "      <td>Scrum Master;Oracle Database Administrator/ Sc...</td>\n",
       "      <td>Scrum Master Scrum Master Scrum Master Richmon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Reference ID  \\\n",
       "0  C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\1.h...   \n",
       "1  C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\10....   \n",
       "2  C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\100...   \n",
       "3  C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\100...   \n",
       "4  C:\\Workspace\\java\\scrape_indeed\\dba_part_1\\100...   \n",
       "\n",
       "                                         Occupations  \\\n",
       "0  Database Administrator;Database Administrator;...   \n",
       "1  Database Administrator;SQL, Microsoft PowerPoi...   \n",
       "2  Oracle Database Administrator;Oracle Database ...   \n",
       "3  Amazon Redshift Administrator and ETL Develope...   \n",
       "4  Scrum Master;Oracle Database Administrator/ Sc...   \n",
       "\n",
       "                                         Text Resume  \n",
       "0  Database Administrator <span class=\"hl\">Databa...  \n",
       "1  Database Administrator <span class=\"hl\">Databa...  \n",
       "2  Oracle Database Administrator Oracle <span cla...  \n",
       "3  Amazon Redshift Administrator and ETL Develope...  \n",
       "4  Scrum Master Scrum Master Scrum Master Richmon...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "for o in df['Occupations'].values:\n",
    "    # if not isinstance(o, str):\n",
    "    #     print((o))\n",
    "\n",
    "    if 'python' in o:\n",
    "\n",
    "        # print((o) )\n",
    "        c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_df = df[df['Occupations'].str.contains('Python', case=False, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'046.html#54'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_df['Reference ID'].iloc[0][44:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_df.to_csv('python_res.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Database Administrator <span class=\"hl\">Database</span> <span class=\"hl\">Administrator</span> sql server database administrator Houston, TX DATABASE ADMINISTRATOR\\xa0 Over five years of experience in database management systems administration, expert at developing and maintaining database, with strong background of working on high-end servers. I would like to establish a fulfilling, proactive career and assume every increasing responsibility in the course of time. Work Experience Database Administrator Intercontinental Registry - Lagos, GU December 2008 to August 2011 Responsibilities\\xa0 Key responsibilities.\\xa0 Planning, Development:\\xa0 Involved in analyzing business requirements, developing and designing data models,\\xa0 \\x95 Provided and designed DB tools to assist in the database management, transactions and processing environments.\\xa0 \\x95 Assessed and executed implementing of new technologies.\\xa0 \\x95 Worked in coordination with company developers and project managers.\\xa0 \\x95 Developed training programs and trained technical support and applications personnel to utilize on-line databases environment process.\\xa0 \\xa0 Technical Support:\\xa0 Monitored performance and capacity to provide resolutions to system problems,\\xa0 \\x95 Provided technical support for SQL database environment by overseeing databases development and organization.\\xa0 \\x95 Communicated regularly with staff to ensure smooth flow of information from SQL database.\\xa0 \\x95 Created database with back-up system.\\xa0 \\xa0 Maintenance:\\xa0 \\x95 Monitored data availability for faster query response by user.\\xa0 \\x95 Designed conceptual schema as per the client needs.\\xa0 \\x95 Looked after the security of company data from external access and threats.\\xa0 \\x95 Tested and installed latest versions of Database Management Systems in firm.\\xa0 \\x95 Granted access to users as per their needs and requirements.\\xa0 \\xa0 Skills Used\\xa0 sql server management studio,visual studio Education bsc in computer science lagos state university - Lagos, GU Skills SQL, Microsoft PowerPoint, Windows XP, Microsoft office, Assembly Language, Microsoft Windows 7, C, C#, Visual basic and java programming ,Microprocessors ,Hardware/Systems Design, Digital Electronics, System organization, Microsoft excel.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k8/70m0bc0n79v7_yxcswv04n3m0000gn/T/ipykernel_28220/3953261526.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  python_df['summary'] = None\n",
      "2024-05-05 00:02:34,649 - 8461253312 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 00:02:36,991 - 8461253312 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 00:02:39,012 - 8461253312 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 00:02:40,166 - 8461253312 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 00:02:41,782 - 8461253312 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 00:03:00,414 - 8461253312 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 00:03:29,007 - 8461253312 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 00:03:54,079 - 8461253312 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 00:04:21,433 - 8461253312 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 00:04:56,677 - 8461253312 - __init__.py-__init__:518 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on 160\n"
     ]
    }
   ],
   "source": [
    "from llm_src import LLMClient\n",
    "import os\n",
    "LLM = LLMClient()\n",
    "llm = LLM.get_llm(model_name='groq')\n",
    "\n",
    "from crewai import Agent, Task, Crew\n",
    "from textwrap import dedent\n",
    "\n",
    "\n",
    "res_summary_agent = a.create_agent('Resume_Summarizer',llm,verbose=False)\n",
    "\n",
    "python_df['summary'] = None\n",
    "\n",
    "\n",
    "\n",
    "for index, row in python_df.iterrows():\n",
    "   print(f'working on {index}')\n",
    "   name = row['Reference ID'][44:]\n",
    "   file_path = f'resume_corpus/python_agents/{name}.txt'\n",
    "   if os.path.exists(file_path) or index == 497:\n",
    "     continue\n",
    "     \n",
    "     \n",
    "   o = row['Occupations']\n",
    "   t = row['Text Resume']\n",
    "   # task = Task(\n",
    "   # description=f'Summarize the following resume {o} {t}',\n",
    "   # expected_output=dedent(f'''A bullet list summary containing name,education,skills and experience\n",
    "   #                         example output:\n",
    "   #                         ----------------------------------------------\n",
    "   #                         Name : jon doe\n",
    "   #                         education : MIT 2014\n",
    "   #                         skills :python,java,.....\n",
    "   #                         experience:\n",
    "   #                         - worked at microsoft\n",
    "   #                            * created edge browser\n",
    "   #                         - worked at apple\n",
    "   #                            * worked on siri\n",
    "                           \n",
    "   #                         -----------------------------------------------\n",
    "   #                         '''),\n",
    "   # agent=res_summary_agent,\n",
    "   # tools=[]\n",
    "   # )\n",
    "\n",
    "\n",
    "   task = Task(\n",
    "   description=f'Summarize the following resume {o} {t}',\n",
    "   expected_output=dedent(f'''A bullet list summary containing name,education,skills and experience\n",
    "                           example output:\n",
    "                           ----------------------------------------------\n",
    "                          role 1:Senior Data Researcher\n",
    "                          goal 1:Uncover cutting-edge developments in \n",
    "                          backstory 1:You're a seasoned researcher with a knack for uncovering the latest\n",
    "                          developments in . Known for your ability to find the most relevant\n",
    "                          information and present it in a clear and concise manner.\n",
    "                          \n",
    "                          role 2:Senior Data Researcher\n",
    "                          goal 2:Uncover cutting-edge developments in \n",
    "                          backstory 2:You're a seasoned researcher with a knack for uncovering the latest\n",
    "                          developments in . Known for your ability to find the most relevant\n",
    "                          information and present it in a clear and concise manner.\n",
    "                           \n",
    "                           -----------------------------------------------\n",
    "                           '''),\n",
    "   agent=res_summary_agent,\n",
    "   tools=[]\n",
    "   )\n",
    "   crew = Crew(\n",
    "               agents=[res_summary_agent],\n",
    "               tasks=[task],\n",
    "               verbose=0\n",
    "         )\n",
    "\n",
    "\n",
    "\n",
    "   result = crew.kickoff()\n",
    "   # python_df.at[index, 'summary'] = result\n",
    "   with open(file_path, 'w') as file:\n",
    "    # Write the string to the file\n",
    "    file.write(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Name : Database Administrator\n",
      "Education : BSc in Computer Science, Lagos State University\n",
      "Skills : SQL, Microsoft PowerPoint, Windows XP, Microsoft Office, Assembly Language, Microsoft Windows 7, C, C#, Visual Basic, Java programming, Microprocessors, Hardware/Systems Design, Digital Electronics, System Organization, Microsoft Excel\n",
      "Experience:\n",
      "- Worked at Intercontinental Registry, Lagos, GU (December 2008 to August 2011)\n",
      "    * Planned, developed, and maintained databases\n",
      "    * Designed and implemented DB tools for database management\n",
      "    * Worked with developers and project managers to implement new technologies\n",
      "    * Developed training programs for technical support and applications personnel\n",
      "    * Provided technical support for SQL database environment\n",
      "    * Monitored performance and capacity to provide resolutions to system problems\n",
      "    * Designed conceptual schema as per client needs\n",
      "    * Ensured data security and granted access to users as per their needs\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------\n",
      "Name : Database Administrator\n",
      "Education : Bachelor of Science, Lead City University, July 2013\n",
      "Skills : Database administration, Database, Ms sql server, Ms sql server 2005, Sql server, Sql server 2005, Sql server 2008, Sql server 2008 r2, Sql server 2012, Sql, Sql queries, Stored procedures, Clustering, Backups, T-sql, Virtualization, R2, Maintenance, Problem solving, Shipping\n",
      "Experience:\n",
      "- Database Administrator, Family Private Care LLC, Lawrenceville, GA, April 2017 to Present\n",
      "    * Confirm that backups have been made and successfully saved to a secure location\n",
      "    * Planning for backup and recovery of database information\n",
      "    * Maintaining archived data\n",
      "    * Backing up and restoring databases\n",
      "    * Contacting database vendor for technical support\n",
      "    * Generating various reports by querying from database as per needed\n",
      "    * Managing and monitoring data replication\n",
      "    * Acting as liaison with users\n",
      "    * High Availability or Disaster Recovery Logs\n",
      "    * Correcting errors and making necessary modifications\n",
      "    * Modifying existing databases and database management systems or directing programmers and analysts to make changes\n",
      "    * Working as part of a project team to coordinate database development and seeing termite project scope and limitations\n",
      "    * Training Users and answering questions\n",
      "    * Approving, scheduling, planning, and supervising the installation and testing of new products and improvements to computer systems\n",
      "    * Reviewing Procedures in Database management system manuals for making changes to database\n",
      "    * Selecting and entering codes to monitor database performance and to create production database\n",
      "    * Checking the backup failure alerts, correcting errors, and rerunning backups\n",
      "    * Reviewing the average duration of backup, any significant changes occurred, and investigating on this\n",
      "    * Validating backup files using restore verify only\n",
      "    * Creating jobs to take care of the task and sending a notification if it fails to verify any backup\n",
      "    * Monitoring all backup and log history, cleaning when designed\n",
      "    * Finding out the newly added databases and defining the backup plan\n",
      "    * Verifying the free space on each drive on all servers\n",
      "    * Researching the cause of the free space fluctuation and resolving if necessary\n",
      "    * Designing a SSRS report to showcase and review the delta values\n",
      "    * Confirming all servers/databases are up and running fine\n",
      "- Database Administrator, Incomm Alpharetta, Alpharetta, GA, January 2014 to February 2017\n",
      "    * Administering and maintaining over 150 database servers of Production and Test environment\n",
      "    * Analyzing the current database environment to determine recommended database maintenance, security, and Microsoft SQL Server best practices\n",
      "    * Monitoring and troubleshooting production environments using Idera SQLdm\n",
      "    * Creating and maintaining documentation for DBA standard operating procedures\n",
      "    * Ensuring that all code changes made in the production environment are SOX compliance before they are deployed\n",
      "    * Analyzing and migrating data using ETL into SQL Server databases to support customer's implementation\n",
      "    * Working closely with infrastructure team for patching and hardware upgrades, and ensuring that both production and test servers are up to date by applying Windows and SQL Server patches\n",
      "    * Upgrading servers as required from SQL Server 2005 to SQL Server 2008, 2012, and 2014\n",
      "    * Completing database administration maintenance projects as required\n",
      "    * Providing 24/7 on-call support as needed\n",
      "    * Coordinating and configuring new nodes for production server clusters for high availability\n",
      "    * Periodically restoring backup files in test environment to check for corruption\n",
      "    * Reviewing security, performance, and disk space and recommending corrective actions where needed\n",
      "    * Assisting application team in the creation of databases, construction of queries, modification of database tables, and troubleshooting data issues\n",
      "    * Monitoring servers for resources utilization (disk space, memory, CPU, etc.)\n",
      "    * Creating and executing a migration/decommission plan for over 500 databases within a 4-month project window\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Analyzed database environment for maintenance best practices, security enhancements, and Microsoft SQL Server optimizations.\n",
      "\n",
      "- Monitored production environments using Idera SQLDM, maintained documentation on DBA standard operating procedures.\n",
      "\n",
      "- Oversaw code changes in the production environment to ensure compliance with Sarbanes-Oxley (SOX) regulations pre-deployment.\n",
      "\n",
      "- Managed data migration and ETL processes into SQL Server databases for customers' implementation needs.\n",
      "\n",
      "- Coordinated with infrastructure teams on patching, upgrades of server operating systems, Windows, and Microsoft SQL Servers to version 2014.\n",
      "\n",
      "- Provided round-the-clock support services as required by the employer.\n",
      "\n",
      "- Assisted in creating new production server clusters for high availability using clustering technology.\n",
      "\n",
      "- Restored databases from backups to check for corruption and implemented necessary fixes.\n",
      "\n",
      "- Designed customized maintenance plans, executed them with maintenance solutions such as SQL Server Agent Jobs, and monitored replication states.\n",
      "\n",
      "- Managed user access levels through the use of system/DB roles and permissions within Microsoft SQL Servers.\n",
      "\n",
      "- Demonstrated proficiency in various SQL versions from 2005 to 2s14 (including support for Always On Availability Groups).\n",
      "\n",
      "- Executed design tasks that included table creation, index management, stored procedure writing, and the execution of business logic within database environments.\n",
      "\n",
      "- Collaborated with development teams on data-related issues in debug/testing phases using T-SQL queries.\n",
      "\n",
      "- Engaged in planning and executing SQL Server 2016 databases for critical operations within a professional setting.\n",
      "\n",
      "- Utilized SQL Profiler tools during setup, troubleshooting, configuration adjustments, capacity planning, performance tuning, backup, and recovery procedures.\n",
      "\n",
      "- Displayed strong analytical skills to perform audits on database efficiency.\n",
      "\n",
      "- Possesses communication skills necessary for collaborating in a team environment with dedication toward meeting project deadlines.\n",
      "\n",
      "- Emphasizes work ethic, problem solving ability, independent operation capacity, and readiness for challenging tasks without the need to use additional tools.\n"
     ]
    }
   ],
   "source": [
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
